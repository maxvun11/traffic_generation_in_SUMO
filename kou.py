# -*- coding: utf-8 -*-
"""kou

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JhNstJN1wYWahAZeiVBTpDu6bcKnbI0b
"""

import matplotlib.pyplot as plt
import numpy as np

# Original box coordinates (J0-J3)
original_nodes = {
    'J0': (0, 0),
    'J1': (200, 0),
    'J2': (200, -200),
    'J3': (0, -200)
}

def node_to_coordinate(node_value):
    """Convert node value to (x,y) coordinates on the 200x200 box"""
    node_value = node_value % 4  # Wrap values outside [0,4)

    if 0 <= node_value <= 1:  # J0 to J1 (top edge)
        x = 200 * node_value
        y = 0
    elif 1 < node_value <= 2:  # J1 to J2 (right edge)
        x = 200
        y = -200 * (node_value - 1)
    elif 2 < node_value <= 3:  # J2 to J3 (bottom edge)
        x = 200 - 200 * (node_value - 2)
        y = -200
    else:  # 3 < node_value < 4 (J3 to J0, left edge)
        x = 0
        y = -200 + 200 * (node_value - 3)
    return (round(x, 2), round(y, 2))

# # Your filtered routes (±0.3 from real nodes)
routes = [
    [ 2.0192,  2.9557, -0.0317,  1.1390],   # Route 1
    [ 0.9994,  1.9249,  3.0550,  0.0129],   # Route 2
    [-0.0472,  3.0526,  2.0289,  0.9768],   # Route 3
    [ 2.0456,  1.0654, -0.0385,  3.0205],   # Route 4
    [ 2.9677,  1.8781,  0.9967,  0.1073],   # Route 5
    [ 3.0249, -0.1944,  1.0684,  2.1756],   # Route 6
    [2.0848, 3.0536, -0.0908, 1.0063],
    [0.9787, 2.0409, 3.2027, -0.1076],
]

# Create figure
plt.figure(figsize=(12, 12))

# Plot original box
box_x = [0, 200, 200, 0, 0]
box_y = [0, 0, -200, -200, 0]
plt.plot(box_x, box_y, 'k-', linewidth=3, label='Original Box')
plt.scatter(box_x[:4], box_y[:4], c='red', s=150, zorder=5,
            edgecolors='black', label='Real Nodes (0,1,2,3)')

# Plot each route
colors = plt.cm.tab20(np.linspace(0, 1, len(routes)))
for i, route in enumerate(routes):
    # Convert node values to coordinates
    coords = [node_to_coordinate(val) for val in route]
    x = [c[0] for c in coords]
    y = [c[1] for c in coords]

    # Close the loop
    x.append(x[0])
    y.append(y[0])

    # Plot with arrow markers to show direction
    plt.plot(x, y, 'o-', color=colors[i], linewidth=2.5,
             markersize=10, markeredgecolor='black',
             label=f'Route {i+1}')

    # Add small arrows to show direction
    for j in range(len(x)-1):
        dx = x[j+1] - x[j]
        dy = y[j+1] - y[j]
        plt.arrow(x[j], y[j], dx*0.8, dy*0.8,
                  shape='full', lw=0, length_includes_head=True,
                  head_width=8, head_length=10, color=colors[i])

# Format plot
plt.title('4-Point Routes on Box Network',
          fontsize=14, pad=20)
plt.xlabel('X Position', fontsize=12)
plt.ylabel('Y Position', fontsize=12)
plt.legend(bbox_to_anchor=(1.15, 1), loc='upper left', fontsize=10)
plt.grid(True, linestyle='--', alpha=0.5)
plt.axis('equal')
plt.xlim(-50, 250)
plt.ylim(-250, 50)

# Add node value annotations
for i, node in enumerate(['J0 (0, 0)', 'J1 (200, 0)', 'J2 (200, -200)', 'J3 (0, -200)']):
    plt.annotate(node, (box_x[i]+10, box_y[i]+10), fontsize=12,
                bbox=dict(facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()

import numpy as np

# Valid routes (both clockwise and counter-clockwise)
valid_routes = [
    [0, 1, 2, 3],
    [1, 2, 3, 0],
    [2, 3, 0, 1],
    [3, 0, 1, 2],
    [0, 3, 2, 1],
    [3, 2, 1, 0],
    [2, 1, 0, 3],
    [1, 0, 3, 2]
]
# valid_routes = [
#     [ 0,  1,  2 ],
#     [ 1,  2,  0 ],
#     [ 2,  0,  1 ],
#     [0, 2, 1 ],
#     [1, 0, 2 ],
#     [2, 1, 0 ]
# ]


#GAN
# generated_routes = [
#     [-0.00764117, 0.97408164, 2.222679, 3.138826],
#     [-0.00764117, 0.97408164, 2.222679, 3.138826],
#     [0.90305567, 1.9087543, 2.9026074, -0.0753918],
#     [-0.02098873, 3.1089025, 2.0219007, 1.0511523],
#     [0.00756493, 3.0159824, 2.0112956, 1.1288974],
#     [0.00694031, 2.9498165, 1.9910183, 1.2380943],
#     [-0.03237972, 3.1520097, 2.029616, 1.0959333],
#     [0.9365567, 1.9211503, 2.971211, -0.05523023]
# ]

# GAN use this 80.87%
# generated_routes = [
#     [0.85622585, 1.8795379, 2.9122195, 0.07764352],
#     [2.4957461, 2.289731, 0.64773834, 0.35258457],
#     [2.948221, 1.1888067, 0.91429734, 0.78521764],
#     [2.146386, -0.20310114, 1.9521908, 2.233095],
#     [0.90217966, 2.0111656, 3.145435, -0.00525623],
#     [2.9164572, 0.5400751, 0.8003247, 1.3685013],
#     [0.918273, 2.0019495, 2.9746368, -0.02240881],
#     [-0.2490361, 3.508185, 1.7412034, 1.4151525]
# ]

#GAN 81.38
# generated_routes = [
#     [-0.5662179,  3.4247327,  3.0284972,  0.91964877],     # Route 1
#     [ 3.1469877,  2.1257951,  1.0327024, -0.00023770332],  # Route 2
#     [ 2.5733347,  1.341307,   0.8456905,  0.44537163],     # Route 3
#     [ 2.442262,   0.5645396,  1.0148703,  1.1614485],      # Route 4
#     [ 1.9649938,  2.6464918,  0.15765911, 0.87277263],     # Route 5
#     [ 2.7854748, -0.6142654,  0.98536015, 2.3607256],      # Route 6
#     [ 3.0328264,  1.9879524,  0.93325996, 0.11864585],     # Route 7
#     [ 2.6136627,  1.9720922,  0.7864181,  0.13794422]      # Route 8
# ]

#GAN 84.23
# generated_routes = [
#     [2.0520368, 3.053995, 0.04717731, 0.9860088],
#     [2.8507366, 1.9431129, 0.93762976, -0.03855392],
#     [2.131301, 0.58837056, 0.05851112, 3.5337768],
#     [1.8511055, 1.5407033, 0.01477587, 2.1420717],
#     [2.3229327, 1.6949897, 1.3561361, -0.08026415],
#     [0.35628873, 2.2218063, 3.9677093, 0.00970779],
#     [2.6968358, 2.1111834, 0.7756759, 0.1356483],
#     [1.8510368, 2.805898, 0.00850511, 0.990231]
# ]


#WGAN 77.48%
# generated_routes = [
#     [2.8973, 0.3744, 1.1562, 1.8555],   # Route 1
#     [2.6528, 0.4481, 0.6741, 2.2700],   # Route 2
#     [0.7297, 1.0921, 1.3720, 2.8241],   # Route 3
#     [2.8393, 0.1630, 1.3201, 1.8913],   # Route 4
#     [0.3543, 0.9773, 1.8441, 2.8824],   # Route 5
#     [0.7076, 2.7603, 2.1067, 0.4201],   # Route 6
#     [1.2166, 2.3582, 0.7239, 1.9937],   # Route 7
#     [0.1917, 2.6636, 1.9189, 1.2264]    # Route 8
# ]

# #WGAN 80.91
# generated_routes = [
#     [0.4281, 1.0826, 1.6116, 2.8502],
#     [0.3697, 0.6064, 2.0571, 2.7601],
#     [0.448, 2.5534, 1.9957, 0.3647],
#     [0.3057, 2.4673, 2.2787, 0.5288],
#     [2.8987, 1.9125, 1.0479, 0.3946],
#     [0.2815, 2.8749, 1.6194, 1.0957],
#     [2.4753, 2.3497, 0.5285, 0.3132],
#     [0.1421, 1.1648, 2.0611, 2.6092]
# ]

#WGAN 83.22%
# generated_routes = [
#     [2.0848, 3.0536, -0.0908, 1.0063],
#     [0.9787, 2.0409, 3.2027, -0.1076],
#     [-0.1033, 3.117, 2.0839, 0.9889],
#     [2.084, 1.072, -0.0321, 3.1031],
#     [2.9525, 1.1613, 0.9969, 0.8054],
#     [2.9959, -0.1957, 1.0381, 2.2235],
#     [1.8867, 1.2942, 0.2301, 2.7139],
#     [2.1036, 1.045, 1.9783, 1.1576]
# ]

# WGAN use this 87.40%
generated_routes = [
    [ 2.0192,  2.9557, -0.0317,  1.1390],   # Route 1
    [ 0.9994,  1.9249,  3.0550,  0.0129],   # Route 2
    [-0.0472,  3.0526,  2.0289,  0.9768],   # Route 3
    [ 2.0456,  1.0654, -0.0385,  3.0205],   # Route 4
    [ 2.9677,  1.8781,  0.9967,  0.1073],   # Route 5
    [ 3.0249, -0.1944,  1.0684,  2.1756],   # Route 6
    [ 1.9785,  1.4740,  0.0645,  2.5650],   # Route 7
    [ 1.9563,  0.9698,  2.1636,  1.1367]    # Route 8
]

# generated_routes = [
#     [0.0882, 1.9689, 1.1145, ],
#     [1.9803, 0.0301, 1.0115, ],
#     [0.0104, 1.8227, 1.2692, ],
#     [1.1431, 1.9129, 0.0128, ],
#     [0.1127, 0.9207, 1.9768, ],
#     [0.0114, 2.0290, 1.0600, ],
#     [1.7514, 0.7790, 0.5251, ],
#     [1.9998, 0.0770, 0.9560, ]
# ]



def snap_route_to_valid(route):
    route = np.array(route)
    best_perm = None
    best_error = float('inf')
    for perm in valid_routes:
        perm = np.array(perm)
        error = np.mean(np.abs(route - perm))
        if error < best_error:
            best_perm = perm.tolist()
            best_error = error
    return best_perm

def calculate_route_accuracy(route):
    snapped = snap_route_to_valid(route)

    # Calculate accuracy using your formula
    sum_diff = sum(abs(g - s) for g, s in zip(route, snapped))
    sum_snap = sum(snapped)

    if sum_snap == 0:
        accuracy_pct = 0.0
    else:
        accuracy_pct = (sum_snap - sum_diff) / sum_snap * 100.0

    accuracy_pct = max(min(accuracy_pct, 100.0), 0.0)

    # Calculate MAE, RMSE, and sMAPE
    route_arr = np.array(route)
    snapped_arr = np.array(snapped)

    # MAE calculation
    mae = np.mean(np.abs(route_arr - snapped_arr))

    # RMSE calculation
    rmse = np.sqrt(np.mean((route_arr - snapped_arr) ** 2))

    # sMAPE calculation
    epsilon = 1e-6
    numerator = 2 * np.abs(route_arr - snapped_arr)
    denominator = np.abs(snapped_arr) + np.abs(route_arr) + epsilon
    smape = 100 * np.mean(numerator / denominator)

    return accuracy_pct, snapped, mae, rmse, smape

def interpret_smape(smape):
    if smape < 20:
        return "Excellent"
    elif smape < 50:
        return "Good"
    elif smape < 100:
        return "Moderate"
    else:
        return "High Error"

# Evaluate and print results
print("Route Evaluation Report:")
print("-----------------------")
for i, route in enumerate(generated_routes, 1):
    accuracy, snapped, mae, rmse, smape = calculate_route_accuracy(route)

    print(f"Route {i}:")
    print(f"  Generated: {[round(x, 4) for x in route]}")
    print(f"  Snapped:   {snapped}")
    print(f"  Accuracy:  {accuracy:.2f}%")
    print(f"  MAE:       {mae:.4f}")
    print(f"  RMSE:      {rmse:.4f}")
    print(f"  sMAPE:     {smape:.2f}% ({interpret_smape(smape)})")
    print()

# Calculate averages
accuracies = []
maes = []
rmses = []
smapes = []

for route in generated_routes:
    accuracy, _, mae, rmse, smape = calculate_route_accuracy(route)
    accuracies.append(accuracy)
    maes.append(mae)
    rmses.append(rmse)
    smapes.append(smape)

print("\nSummary Statistics:")
print("------------------")
print(f"Average Accuracy: {np.mean(accuracies):.2f}%")
print(f"Average MAE:      {np.mean(maes):.4f}")
print(f"Average RMSE:     {np.mean(rmses):.4f}")
print(f"Average sMAPE:    {np.mean(smapes):.2f}%")
print(f"\nBest sMAPE:       {min(smapes):.2f}%")
print(f"Worst sMAPE:      {max(smapes):.2f}%")

import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from torch.utils.data import DataLoader, TensorDataset

# Reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Real valid routes as training data
valid_routes = [
    [0, 1, 2, 3], [1, 2, 3, 0], [2, 3, 0, 1], [3, 0, 1, 2],
    [0, 3, 2, 1], [3, 2, 1, 0], [2, 1, 0, 3], [1, 0, 3, 2]
]

# Convert real data to tensors
valid_routes_tensor = torch.tensor(valid_routes, dtype=torch.float32).to(device)

# Hyperparameters
input_size = 5
hidden_size = 128
output_size = 4
num_epochs = 10000
batch_size = 4
learning_rate = 0.00004
critic_iterations = 5
lambda_gp = 10.0

# Generator model
class Generator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, output_size)
        )

    def forward(self, x):
        return self.main(x)

# Critic model (no sigmoid)
class Critic(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(Critic, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, 1)
        )

    def forward(self, x):
        return self.main(x)

# Prepare dataset
dataset = TensorDataset(valid_routes_tensor)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

# Initialize models
generator = Generator(input_size, hidden_size, output_size).to(device)
critic = Critic(output_size, hidden_size).to(device)

# Optimizers
g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))
c_optimizer = optim.Adam(critic.parameters(), lr=learning_rate, betas=(0.5, 0.9))

# Gradient penalty function
def gradient_penalty(critic, real_data, fake_data):
    batch_size = real_data.size(0)
    alpha = torch.rand(batch_size, 1, device=device)
    alpha = alpha.expand_as(real_data)
    interpolates = alpha * real_data + (1 - alpha) * fake_data
    interpolates.requires_grad_(True)

    critic_interpolates = critic(interpolates)
    ones = torch.ones(critic_interpolates.size(), device=device)

    gradients = torch.autograd.grad(
        outputs=critic_interpolates,
        inputs=interpolates,
        grad_outputs=ones,
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]

    gradients = gradients.view(batch_size, -1)
    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    return gp

# Training loop
def train_wgan():
    for epoch in range(num_epochs):
        for real_batch in dataloader:
            real_data = real_batch[0].to(device)
            batch_size_curr = real_data.size(0)

            # Train critic
            for _ in range(critic_iterations):
                noise = torch.randn(batch_size_curr, input_size, device=device)
                fake_data = generator(noise).detach()

                c_optimizer.zero_grad()
                # Wasserstein loss + gradient penalty
                loss_real = -critic(real_data).mean()
                loss_fake = critic(fake_data).mean()
                gp = gradient_penalty(critic, real_data, fake_data)
                c_loss = loss_real + loss_fake + lambda_gp * gp
                c_loss.backward()
                c_optimizer.step()

            # Train generator
            g_optimizer.zero_grad()
            noise = torch.randn(batch_size_curr, input_size, device=device)
            fake_data = generator(noise)
            g_loss = -critic(fake_data).mean()
            g_loss.backward()
            g_optimizer.step()

        if epoch % 500 == 0:
            print(f"Epoch [{epoch}/{num_epochs}] - C Loss: {c_loss.item():.4f}, G Loss: {g_loss.item():.4f}")

# Start training
train_wgan()

# Snap function to nearest valid route
def snap_route_to_valid(route):
    route = np.array(route)
    best_perm, best_err = None, float('inf')
    for perm in valid_routes:
        err = np.mean(np.abs(route - np.array(perm)))
        if err < best_err:
            best_perm, best_err = perm, err
    return best_perm

# Generate and print routes
generated_routes = generator(torch.randn(8, input_size, device=device)).cpu().detach().numpy()
print("\nGenerated Routes after WGAN Training:")
for i, route in enumerate(generated_routes, 1):
    snapped = snap_route_to_valid(route)
    print(f"Route {i}: Generated: {np.round(route,4)} | Snapped: {snapped}")

    #87.40 WGAN 83.22% use this

import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from torch.utils.data import DataLoader, TensorDataset

# Reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Real valid routes as training data
valid_routes = [
    [0, 1, 2, 3], [1, 2, 3, 0], [2, 3, 0, 1], [3, 0, 1, 2],
    [0, 3, 2, 1], [3, 2, 1, 0], [2, 1, 0, 3], [1, 0, 3, 2]
]

# Data normalization to [0,1]
valid_routes_np = np.array(valid_routes, dtype=np.float32)
valid_routes_tensor = torch.from_numpy(valid_routes_np / 3.0).to(device)

# Hyperparameters (tuned for faster convergence)
input_size = 5
hidden_size = 128        # reduced capacity
output_size = 4
num_epochs = 10000       # fewer epochs
batch_size = 8           # larger batch size
learning_rate = 1.5e-4     # slightly higher lr
critic_iterations = 3    # fewer critic updates
lambda_gp = 10.0
mse_weight = 3.0         # lower reconstruction weight

# Generator model with moderate depth and LayerNorm
class Generator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, output_size),
            nn.Sigmoid()
        )
    def forward(self, x):
        return self.main(x)

# Critic model without normalization
class Critic(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(Critic, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, 1)
        )
    def forward(self, x):
        return self.main(x)

# Prepare dataset
dataset = TensorDataset(valid_routes_tensor)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

# Initialize models
generator = Generator(input_size, hidden_size, output_size).to(device)
critic = Critic(output_size, hidden_size).to(device)

# Optimizers (Adam)
g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))
c_optimizer = optim.Adam(critic.parameters(), lr=learning_rate, betas=(0.5, 0.9))

# Loss for reconstruction
g_reconstruction_loss = nn.MSELoss()

# Gradient penalty
def gradient_penalty(critic, real_data, fake_data):
    batch_size = real_data.size(0)
    alpha = torch.rand(batch_size, 1, device=device).expand_as(real_data)
    interpolates = alpha * real_data + (1 - alpha) * fake_data
    interpolates.requires_grad_(True)
    out = critic(interpolates)
    grads = torch.autograd.grad(
        outputs=out,
        inputs=interpolates,
        grad_outputs=torch.ones_like(out),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]
    grads = grads.view(batch_size, -1)
    return ((grads.norm(2, dim=1) - 1) ** 2).mean()

# Training loop
for epoch in range(1, num_epochs + 1):
    for real_batch in dataloader:
        real_data = real_batch[0].to(device)
        bsz = real_data.size(0)
        # Critic updates
        for _ in range(critic_iterations):
            noise = torch.randn(bsz, input_size, device=device)
            fake_data = generator(noise).detach()
            c_optimizer.zero_grad()
            c_loss = -critic(real_data).mean() + critic(fake_data).mean()
            c_loss += lambda_gp * gradient_penalty(critic, real_data, fake_data)
            c_loss.backward()
            c_optimizer.step()
        # Generator update
        g_optimizer.zero_grad()
        noise = torch.randn(bsz, input_size, device=device)
        fake_data = generator(noise)
        g_loss = -critic(fake_data).mean() + mse_weight * g_reconstruction_loss(fake_data, real_data)
        g_loss.backward()
        g_optimizer.step()
    # Logging
    if epoch % 500 == 0 or epoch == 1:
        print(f"Epoch {epoch}/{num_epochs} - C_loss: {c_loss.item():.4f}, G_loss: {g_loss.item():.4f}")

# Snap to nearest valid route
def snap_route_to_valid(route):
    route = np.array(route)
    best, best_err = None, float('inf')
    for perm in valid_routes:
        err = np.mean(np.abs(route - np.array(perm)))
        if err < best_err:
            best, best_err = perm, err
    return best

# Generate and print routes
generator.eval()
with torch.no_grad():
    noise = torch.randn(8, input_size, device=device)
    gen = generator(noise).cpu().numpy() * 3.0
print("\nGenerated Routes:")
for i, r in enumerate(gen, 1):
    print(f"Route {i}: {np.round(r,4)} -> {snap_route_to_valid(r)}")

    #77.48 WGAN

import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from torch.utils.data import DataLoader, TensorDataset

# Reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Real valid routes as training data
valid_routes = [
    [0, 1, 2, 3],
    [1, 2, 3, 0],
    [2, 3, 0, 1],
    [3, 0, 1, 2],
    [0, 3, 2, 1],
    [3, 2, 1, 0],
    [2, 1, 0, 3],
    [1, 0, 3, 2]
]

# Hyperparameters
input_size = 5
hidden_size = 128
output_size = 4
num_epochs = 7000
batch_size = 4
learning_rate = 0.01

# Generator model
class Generator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, output_size)
        )

    def forward(self, x):
        return self.main(x)

# Discriminator model
class Discriminator(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(True),
            nn.Linear(hidden_size, output_size),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.main(x)

# Convert real data to tensors
valid_routes_tensor = torch.tensor(valid_routes, dtype=torch.float32).to(device)
dataset = TensorDataset(valid_routes_tensor)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

# Initialize models
generator = Generator(input_size, hidden_size, output_size).to(device)
discriminator = Discriminator(output_size, hidden_size, 1).to(device)

# Loss and optimizers
criterion = nn.BCELoss()
g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))
d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))
g_scheduler = optim.lr_scheduler.StepLR(g_optimizer, step_size=1000, gamma=0.95)
d_scheduler = optim.lr_scheduler.StepLR(d_optimizer, step_size=1000, gamma=0.95)

# Training loop
def train_gan():
    for epoch in range(num_epochs):
        for real_data_batch in dataloader:
            real_data = real_data_batch[0].to(device)
            batch_size_current = real_data.size(0)

            # Add light noise to real data
            noisy_real_data = real_data + 0.05 * torch.randn_like(real_data)

            # Generate fake data
            noise = torch.randn(batch_size_current, input_size, device=device)
            fake_data = generator(noise)

            # Soft labels
            real_labels = torch.full((batch_size_current, 1), 0.9, device=device)
            fake_labels = torch.full((batch_size_current, 1), 0.1, device=device)

            # Train discriminator
            d_optimizer.zero_grad()
            real_pred = discriminator(noisy_real_data)
            fake_pred = discriminator(fake_data.detach())
            d_loss = criterion(real_pred, real_labels) + criterion(fake_pred, fake_labels)
            d_loss.backward()
            d_optimizer.step()

            # Train generator
            g_optimizer.zero_grad()
            fake_pred_for_g = discriminator(fake_data)
            g_loss = criterion(fake_pred_for_g, real_labels)
            g_loss.backward()
            g_optimizer.step()

        g_scheduler.step()
        d_scheduler.step()

        if epoch % 500 == 0:
            print(f"Epoch [{epoch}/{num_epochs}] - D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}")

# Start training
train_gan()

# Generate routes
num_generated_routes = 8
noise = torch.randn(num_generated_routes, input_size, device=device)
generated_routes = generator(noise).cpu().detach().numpy()

print("\nGenerated Routes after GAN Training:")
for i, route in enumerate(generated_routes):
    print(f"Route {i+1}: {route}")

    #94.41%

import torch, torch.nn as nn, torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np, random

# Reproducibility ---------------------------------------------------
torch.manual_seed(42);  np.random.seed(42);  random.seed(42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ------------------------------------------------------------------
valid_routes = np.array([
    [0,1,2,3],[1,2,3,0],[2,3,0,1],[3,0,1,2],
    [0,3,2,1],[3,2,1,0],[2,1,0,3],[1,0,3,2]
], dtype=np.float32)
loader = DataLoader(TensorDataset(torch.tensor(valid_routes)),
                    batch_size=4, shuffle=True, drop_last=True)

# ------------------------------------------------------------------
z_dim, hidden, out_dim = 5, 128, 4

def weights_init(m):
    if isinstance(m, nn.Linear):
        nn.init.normal_(m.weight, 0.0, 0.02)
        nn.init.zeros_(m.bias)

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(z_dim, hidden),          nn.BatchNorm1d(hidden),
            nn.ReLU(True),
            nn.Linear(hidden, hidden),         nn.BatchNorm1d(hidden),
            nn.ReLU(True),
            nn.Linear(hidden, out_dim)
        )
        self.apply(weights_init)
    def forward(self, z):  return self.net(z)

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(out_dim, hidden), nn.LeakyReLU(0.2, True),
            nn.Linear(hidden, hidden),  nn.LeakyReLU(0.2, True),
            nn.Linear(hidden, 1)                # no Sigmoid here!
        )
        self.apply(weights_init)
    def forward(self, x):  return self.net(x).view(-1)

G, D = Generator().to(device), Discriminator().to(device)

# ------------------------------------------------------------------
criterion = nn.BCEWithLogitsLoss()
g_opt = optim.Adam(G.parameters(), lr=2e-4, betas=(0.0, 0.9))
d_opt = optim.Adam(D.parameters(), lr=1e-4, betas=(0.0, 0.9))

epochs, k_d_steps = 9500, 2                    # k_d_steps = critic steps per G step
start_sigma, end_sigma = 0.05, 0.0             # instance‑noise annealing

# ------------------------------------------------------------------
for epoch in range(epochs):
    sigma = start_sigma + (end_sigma - start_sigma) * (epoch / epochs)
    for real_batch, in loader:
        real = real_batch.to(device)

        # ------------------------------------------------- Train D
        for _ in range(k_d_steps):
            z    = torch.randn(real.size(0), z_dim, device=device)
            fake = G(z).detach()

            # add very small Gaussian noise ("instance‑noise")
            noisy_real = real + sigma*torch.randn_like(real)
            noisy_fake = fake + sigma*torch.randn_like(fake)

            d_opt.zero_grad()

            # one‑sided label‑smoothing + 5% random flips
            real_lbl = torch.empty(real.size(0), device=device).uniform_(0.85,0.95)
            fake_lbl = torch.zeros(real.size(0), device=device)
            flip_idx = torch.rand(real.size(0), device=device) < 0.05
            real_lbl[flip_idx], fake_lbl[flip_idx] = 0.0, real_lbl[flip_idx]

            loss_d = (criterion(D(noisy_real), real_lbl)
                    + criterion(D(noisy_fake), fake_lbl)) / 2
            loss_d.backward(); d_opt.step()

        # ------------------------------------------------- Train G
        z    = torch.randn(real.size(0), z_dim, device=device)
        fake = G(z)
        g_opt.zero_grad()
        loss_g = criterion(D(fake), torch.ones(real.size(0), device=device))
        loss_g.backward(); g_opt.step()

    # ------------- simple console log -----------------------------
    if epoch % 500 == 0:
        print(f"Epoch {epoch:>4} | D loss {loss_d.item():.4f} | G loss {loss_g.item():.4f}")

# ------------------------------------------------------------------ sampling
with torch.no_grad():
    z = torch.randn(8, z_dim, device=device)
    routes = G(z).cpu().numpy()
print("\nGenerated routes:")
for i,r in enumerate(routes,1):
    print(f"Route {i}: {r}")

    # 80.87 GAN, 81.38 GAN

import matplotlib.pyplot as plt
import numpy as np

# Original box coordinates (J0-J3)
original_nodes = {
    'J0': (0, 0),
    'J1': (200, 0),
    'J2': (200, -200),
    'J3': (0, -200)
}

def node_to_coordinate(node_value):
    """Convert node value to (x,y) coordinates on the 200x200 box"""
    node_value = node_value % 4  # Wrap values outside [0,4)

    if 0 <= node_value <= 1:  # J0 to J1 (top edge)
        x = 200 * node_value
        y = 0
    elif 1 < node_value <= 2:  # J1 to J2 (right edge)
        x = 200
        y = -200 * (node_value - 1)
    elif 2 < node_value <= 3:  # J2 to J3 (bottom edge)
        x = 200 - 200 * (node_value - 2)
        y = -200
    else:  # 3 < node_value < 4 (J3 to J0, left edge)
        x = 0
        y = -200 + 200 * (node_value - 3)
    return (round(x, 2), round(y, 2))

# Create figure
plt.figure(figsize=(12, 12))

# Plot original box with coordinate labels
box_x = [0, 200, 200, 0, 0]
box_y = [0, 0, -200, -200, 0]
plt.plot(box_x, box_y, 'k-', linewidth=3, label='Original Box')

# Plot nodes with coordinate labels
node_labels = [
    ('J0 (0, 0)', (0, 0)),
    ('J1 (200, 0)', (200, 0)),
    ('J2 (200, -200)', (200, -200)),
    ('J3 (0, -200)', (0, -200))
]

# Plot nodes and add annotations
for label, (x, y) in node_labels:
    plt.scatter(x, y, c='red', s=150, edgecolors='black', zorder=5)
    plt.annotate(label,
                 (x + 10, y + 10),
                 fontsize=12,
                 bbox=dict(facecolor='white', alpha=0.8))

# Rest of the plot formatting
plt.title('Filtered Routes (±0.3 from Real Nodes)\n200x200 Box Network',
          fontsize=14, pad=20)
plt.xlabel('X Position', fontsize=12)
plt.ylabel('Y Position', fontsize=12)
plt.grid(True, linestyle='--', alpha=0.5)
plt.axis('equal')
plt.xlim(-50, 250)
plt.ylim(-250, 50)
plt.tight_layout()
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from torch.utils.data import DataLoader, TensorDataset
import time

# Reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

# Dataset
base_routes = np.array([
    [0.0, 1.0, 2.0],
    [1.0, 2.0, 0.0],
    [2.0, 0.0, 1.0],
    [0.0, 2.0, 1.0],
    [1.0, 0.0, 2.0],
    [2.0, 1.0, 0.0]
], dtype=np.float32)

valid_routes = np.tile(base_routes, (100, 1)) / 2.0  # normalize to [-1, 1]
loader = DataLoader(TensorDataset(torch.tensor(valid_routes)), batch_size=64, shuffle=True)

# Architecture
z_dim, hidden, out_dim = 5, 128, 3

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(z_dim, hidden),
            nn.ReLU(True),
            nn.Linear(hidden, hidden),
            nn.ReLU(True),
            nn.Linear(hidden, out_dim)
        )

    def forward(self, z):
        return self.model(z)

class Critic(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(out_dim, hidden),
            nn.ReLU(True),
            nn.Linear(hidden, hidden),
            nn.ReLU(True),
            nn.Linear(hidden, 1)
        )

    def forward(self, x):
        return self.model(x)

G = Generator().to(device)
C = Critic().to(device)

g_opt = optim.Adam(G.parameters(), lr=1e-4, betas=(0.0, 0.9))
c_opt = optim.Adam(C.parameters(), lr=1e-4, betas=(0.0, 0.9))

# Gradient Penalty
def gradient_penalty(critic, real, fake):
    batch_size = real.size(0)
    epsilon = torch.rand(batch_size, 1, device=device).expand_as(real)
    interpolated = epsilon * real + (1 - epsilon) * fake
    interpolated.requires_grad_(True)
    scores = critic(interpolated)
    gradients = torch.autograd.grad(
        outputs=scores,
        inputs=interpolated,
        grad_outputs=torch.ones_like(scores),
        create_graph=True,
        retain_graph=True,
    )[0]
    gradients = gradients.view(batch_size, -1)
    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    return gp

# Training
epochs = 300
critic_steps = 5
lambda_gp = 10

for epoch in range(epochs):
    for real_batch, in loader:
        real = real_batch.to(device)
        batch_size = real.size(0)

        # Train critic
        for _ in range(critic_steps):
            z = torch.randn(batch_size, z_dim, device=device)
            fake = G(z).detach()
            c_real = C(real).mean()
            c_fake = C(fake).mean()
            gp = gradient_penalty(C, real, fake)
            c_loss = -(c_real - c_fake) + lambda_gp * gp

            c_opt.zero_grad()
            c_loss.backward()
            c_opt.step()

        # Train generator
        z = torch.randn(batch_size, z_dim, device=device)
        fake = G(z)
        g_loss = -C(fake).mean()

        g_opt.zero_grad()
        g_loss.backward()
        g_opt.step()

    if epoch % 50 == 0:
        print(f"Epoch {epoch}: Critic Loss = {c_loss.item():.4f}, Generator Loss = {g_loss.item():.4f}")

# Generation
with torch.no_grad():
    z = torch.randn(100, z_dim, device=device)
    generated_routes = G(z).cpu().numpy() * 2.0  # denormalize to [-2, 2]

# Matching Function
def exact_vector_match(vec1, vec2):
    return np.allclose(vec1, vec2, atol=1e-3) and np.all(np.signbit(vec1) == np.signbit(vec2))

match_count = 0
for x in generated_routes:
    for v in base_routes:
        if exact_vector_match(x, v):
            match_count += 1
            break

match_percent = match_count / len(generated_routes) * 100

print("\nSample Generated Routes:")
for i, route in enumerate(generated_routes[:8], 1):
    print(f"Route {i}: {np.round(route, 3)}")

print(f"\n✅ Match to valid base route (sign-sensitive): {match_percent:.2f}%")